{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisha1729/lottery-ticket/blob/master/Conv2_iterative_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk7TBCHzKmHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install inferno-pytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZaP4uS3aBqs0",
        "outputId": "da834308-f999-4eb3-fbf1-6f5de4e61912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1729
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from inferno.extensions.layers.reshape import Flatten\n",
        "from builtins import range\n",
        "from math import sqrt, ceil\n",
        "  \n",
        "  \n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 1e-3)\n",
        "        m.bias.data.fill_(0.)\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "#--------------------------------\n",
        "# Device configuration\n",
        "#--------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: %s'%device)\n",
        "\n",
        "#--------------------------------\n",
        "# Hyper-parameters\n",
        "#--------------------------------\n",
        "input_size = 3\n",
        "num_classes = 10\n",
        "hidden_size = [64,128,256,512]\n",
        "fc_size = 256\n",
        "num_epochs = 1\n",
        "batch_size = 200\n",
        "learning_rate = 2e-3\n",
        "learning_rate_decay = 0.95\n",
        "reg=0.001\n",
        "num_training= 49000\n",
        "num_validation =1000\n",
        "norm_layer = None\n",
        "prune_percent = 20\n",
        "prune_iter = 50\n",
        "\n",
        "\n",
        "#  Initialising the initial mask with all ones\n",
        "mask_layerwise = {};\n",
        "\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Load the CIFAR-10 dataset\n",
        "#-------------------------------------------------\n",
        "data_aug_transforms = []\n",
        "\n",
        "norm_transform = transforms.Compose(data_aug_transforms+[transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                     ])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                     ])\n",
        "cifar_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
        "                                           train=True,\n",
        "                                           transform=norm_transform,\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='datasets/',\n",
        "                                          train=False,\n",
        "                                          transform=test_transform\n",
        "                                          )\n",
        "#-------------------------------------------------\n",
        "# Prepare the training and validation splits\n",
        "#-------------------------------------------------\n",
        "mask = list(range(num_training))\n",
        "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Data loader\n",
        "#-------------------------------------------------\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "#-------------------------------------------------\n",
        "# Convolutional neural network\n",
        "#-------------------------------------------------\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, num_classes, norm_layer=None):\n",
        "        super(ConvNet, self).__init__()\n",
        "        #######################################################################################################\n",
        "        # The Conv-2, Conv-4, and Conv-6 architectures are variants of the VGG (Simonyan & Zisserman,\n",
        "        # 2014) network architecture scaled down for the CIFAR10 (Krizhevsky & Hinton, 2009) dataset. Like\n",
        "        # VGG, the networks consist of a series of modules. Each module has two layers of 3x3 convolutional\n",
        "        # filters followed by a maxpool layer with stride 2. After all of the modules are two fully-connected\n",
        "        # layers of size 256 followed by an output layer of size 10; in VGG, the fully-connected layers are of\n",
        "        # size 4096 and the output layer is of size 1000. Like VGG, the first module has 64 convolutions in\n",
        "        # each layer, the second has 128, the third has 256, etc. The Conv-2, Conv-4, and Conv-6 architectures\n",
        "        # have 1, 2, and 3 modules, respectively.\n",
        "        #######################################################################################################\n",
        "        layers = []\n",
        "        \n",
        "        # 1 module for Conv-2\n",
        "        layers.append(nn.Conv2d(input_size, 64 , kernel_size = 3, stride = 1, padding = 1))\n",
        "#         layers.append(nn.BatchNorm2d(64))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Conv2d(64, 64 , kernel_size = 3, stride = 1, padding = 1))\n",
        "#         layers.append(nn.BatchNorm2d(64))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0))\n",
        "        \n",
        "        # Fully Connected Layers\n",
        "        layers.append(Flatten())\n",
        "        \n",
        "        layers.append(nn.Linear(16384, fc_size))\n",
        "        layers.append(nn.Linear(fc_size, fc_size))\n",
        "        \n",
        "        # Output Layer\n",
        "        layers.append(nn.Linear(fc_size, num_classes))\n",
        "        \n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        for name,param in model.named_parameters():\n",
        "          if 'weight' in name:\n",
        "            new_param = param*mask_layerwise[name].float()\n",
        "#             if '0' in name:\n",
        "#               print(\"forward prun\")\n",
        "#               print(new_param)\n",
        "#               break\n",
        "            model.state_dict()[name].data.copy_(new_param)\n",
        "        return out\n",
        "      \n",
        "def PrintModelSize(model, disp=True):\n",
        "    model_sz = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Number of trainable parameters = \", model_sz)\n",
        "    return model_sz\n",
        "\n",
        "  \n",
        "def VisualizeFilter(model):\n",
        "    w = model.layers[0].weight\n",
        "    w_grid = torchvision.utils.make_grid(w,8, normalize = True, scale_each = True)\n",
        "    w_grid = w_grid.permute(2, 1, 0)\n",
        "    plt.imshow(w_grid.detach().cpu().numpy())    \n",
        "    return w_grid;\n",
        "  \n",
        "def prune(prune_percent, model, mask_layerwise):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            sort_array = torch.sort(torch.abs(torch.masked_select(param, mask_layerwise[name]))).values\n",
        "            thres_index = int(prune_percent*len(sort_array)/100)\n",
        "            threshold = sort_array[thres_index]\n",
        "            new_mask = torch.where(torch.abs(param).cpu() <= threshold.cpu(), torch.zeros(mask_layerwise[name].shape).byte().cpu(), mask_layerwise[name].cpu()).byte().cuda()\n",
        "            mask_layerwise[name] = new_mask        # Updating mask\n",
        "#             print(mask_layerwise[name])\n",
        "#     for name, param in model.named_parameters():\n",
        "      \n",
        "#       print(param)\n",
        "#       break\n",
        "\n",
        "  \n",
        "model = ConvNet(input_size, hidden_size, num_classes, norm_layer=norm_layer).to(device)\n",
        "model.apply(weights_init)\n",
        "\n",
        "# Initialising mask with all ones\n",
        "for name, param in model.named_parameters():\n",
        "    mask_layerwise[name] = (torch.ones(param.size()).byte().to(device))\n",
        "\n",
        "# Print the model\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Print model size\n",
        "PrintModelSize(model)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
        "loss_history = []\n",
        "\n",
        "# Train the model\n",
        "lr = learning_rate\n",
        "max_val_acc = 0\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    \n",
        "    # TRAINING\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        train_accuracy.append(100*correct/total)\n",
        "    \n",
        "    # Code to update the lr\n",
        "    lr *= learning_rate_decay\n",
        "    update_lr(optimizer, lr)\n",
        "    model.eval()\n",
        "    \n",
        "    # VALIDATION\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        val_accuracy.append(100*correct/total)\n",
        "\n",
        "        print('Validataion accuracy is: {} %'.format(100 * correct / total))\n",
        "        \n",
        "        # Saving best model\n",
        "        if(correct/total>max_val_acc):\n",
        "          print(\"Saving the model...\")\n",
        "          torch.save(model.state_dict(), 'model_early.ckpt')\n",
        "          max_val_acc = correct/total\n",
        "\n",
        "    model.train()\n",
        "\n",
        "# plt.plot(loss_history)\n",
        "\n",
        "# TESTING\n",
        "\n",
        "# Test the model before pruning\n",
        "model.eval()\n",
        "\n",
        "# Load the best model\n",
        "best_model = torch.load(\"model_early.ckpt\")\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        if total == 1000:\n",
        "            break\n",
        "\n",
        "    print('Accuracy of the network before pruning on the {} test images: {} %'.format(total, 100 * correct / total))\n",
        "\n",
        "print(\"*********Pruning***********\")\n",
        "prune_accuracy = []\n",
        "for iter in range(prune_iter):\n",
        "\n",
        "  # Pruning the network\n",
        "  prune(prune_percent, model, mask_layerwise)\n",
        "\n",
        "  # Print first param tensor\n",
        "#   for param in model.parameters():\n",
        "#     print(param)\n",
        "#     break\n",
        "for parma in model.paranter\n",
        "  # Test the model after pruning\n",
        "  with torch.no_grad():\n",
        "\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "          if total == 1000:\n",
        "              break\n",
        "      acc = 100 * correct / total\n",
        "      print('Accuracy of the network after pruning on the {} test images: {} %'.format(total, acc))\n",
        "      print(\"Iteration no: %d, Percent = %d\", iter, prune_percent)\n",
        "\n",
        "  prune_accuracy.append(acc)\n",
        "plt.plot(prune_accuracy, label = \"Validation\")\n",
        "# plt.plot(train_accuracy, label = \"Train\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "ConvNet(\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Flatten()\n",
            "    (6): Linear(in_features=16384, out_features=256, bias=True)\n",
            "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Number of trainable parameters =  4301642\n",
            "Epoch [1/1], Step [100/245], Loss: 1.4773\n",
            "Epoch [1/1], Step [200/245], Loss: 1.1658\n",
            "Validataion accuracy is: 54.7 %\n",
            "Saving the model...\n",
            "Accuracy of the network before pruning on the 1000 test images: 55.8 %\n",
            "*********Pruning***********\n",
            "Accuracy of the network after pruning on the 1000 test images: 55.4 %\n",
            "Iteration no: %d, Percent = %d 0 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 54.4 %\n",
            "Iteration no: %d, Percent = %d 1 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 53.2 %\n",
            "Iteration no: %d, Percent = %d 2 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 44.9 %\n",
            "Iteration no: %d, Percent = %d 3 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 37.5 %\n",
            "Iteration no: %d, Percent = %d 4 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 36.3 %\n",
            "Iteration no: %d, Percent = %d 5 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 25.2 %\n",
            "Iteration no: %d, Percent = %d 6 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 17.8 %\n",
            "Iteration no: %d, Percent = %d 7 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 17.8 %\n",
            "Iteration no: %d, Percent = %d 8 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 17.2 %\n",
            "Iteration no: %d, Percent = %d 9 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 14.2 %\n",
            "Iteration no: %d, Percent = %d 10 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 11.0 %\n",
            "Iteration no: %d, Percent = %d 11 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.0 %\n",
            "Iteration no: %d, Percent = %d 12 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.9 %\n",
            "Iteration no: %d, Percent = %d 13 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.5 %\n",
            "Iteration no: %d, Percent = %d 14 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 15 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 16 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.2 %\n",
            "Iteration no: %d, Percent = %d 17 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 18 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 19 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 20 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 21 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 22 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 23 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 24 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 25 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 26 20\n",
            "Accuracy of the network after pruning on the 1000 test images: 10.3 %\n",
            "Iteration no: %d, Percent = %d 27 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-64750db82ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m   \u001b[0;31m# Pruning the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m   \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_layerwise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0;31m# Print first param tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-64750db82ba6>\u001b[0m in \u001b[0;36mprune\u001b[0;34m(prune_percent, model, mask_layerwise)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0msort_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_layerwise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mthres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprune_percent\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthres_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mnew_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_layerwise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_layerwise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mmask_layerwise\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mask\u001b[0m        \u001b[0;31m# Updating mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TyYmlxuXC6S",
        "colab_type": "code",
        "outputId": "fcb2c71a-bfd6-4a1d-92f7-113a4d312a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "  \n",
        "def prune(prune_percent, model, mask_layerwise):\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            sort_array = torch.sort(torch.abs(torch.masked_select(param, mask_layerwise[name]))).values\n",
        "            thres_index = int(prune_percent*len(sort_array)/100)\n",
        "            threshold = sort_array[thres_index]\n",
        "            new_mask = torch.where(torch.abs(param).cpu() <= threshold.cpu(), torch.zeros(mask_layerwise[name].shape).byte().cpu(), mask_layerwise[name].cpu()).byte().cuda()\n",
        "            mask_layerwise[name] = new_mask        # Updating mask\n",
        "    for name, param in model.named_parameters():\n",
        "      print(mask_layerwise[name])\n",
        "      break\n",
        "\n",
        "print(\"*********Pruning***********\")\n",
        "prune_accuracy = []\n",
        "\n",
        "for iter in range(prune_iter):\n",
        "\n",
        "  # Pruning the network\n",
        "  prune(prune_percent, model, mask_layerwise)\n",
        "\n",
        "  # Print first param tensor\n",
        "  for param in model.parameters():\n",
        "    print(param)\n",
        "    break\n",
        "\n",
        "  # Test the model after pruning\n",
        "  with torch.no_grad():\n",
        "\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "          if total == 1000:\n",
        "              break\n",
        "      acc = 100 * correct / total\n",
        "      print('Accuracy of the network after pruning on the {} test images: {} %'.format(total, acc))\n",
        "      print(\"Iteration no: %d, Percent = %d\", iter, prune_percent)\n",
        "\n",
        "  prune_accuracy.append(acc)\n",
        "plt.plot(prune_accuracy, label = \"Validation\")\n",
        "# plt.plot(train_accuracy, label = \"Train\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f1bb4e5c0520>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    prune(prune_percent, model, mask_layerwise)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfmI0dN4IO6P",
        "colab_type": "code",
        "outputId": "5473feba-880b-4ae0-f6c8-b6525d359609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(param.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjjMkYqRRVij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# current_mask = torch.ones(param.size()).to(device)\n",
        "percent = 10\n",
        "# for param in model.named_parameters():\n",
        "print(\"Before pruning\")\n",
        "print(param)\n",
        "mask = torch.ones(param.size()).byte().to(device)\n",
        "print(type(param))\n",
        "print(type(mask))\n",
        "print(type(torch.zeros(mask.shape)))\n",
        "sort_array = torch.sort(torch.abs(torch.masked_select(param, mask))).values\n",
        "print(\"Sort aray = \", sort_array)\n",
        "thres_index = int(percent*len(sort_array)/100)\n",
        "print(\"Thres index = \", thres_index)\n",
        "threshold = sort_array[thres_index]\n",
        "# new_mask = param.le(threshold).float()\n",
        "mask = mask.cpu()\n",
        "print(torch.zeros(mask.shape).byte().cpu().is_cuda)\n",
        "print(mask.is_cuda)\n",
        "new_mask = torch.where(torch.abs(param).cpu() <= threshold.cpu(), torch.zeros(mask.shape).byte().cpu(), mask.cpu()).float().cuda()\n",
        "param_new = param*new_mask\n",
        "print(\"Threshold = \", threshold)\n",
        "print(new_mask)\n",
        "print(\"After pruning\")\n",
        "print(param_new)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m51lvYwYIiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mask_layerwise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHZoXAYfY_A8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxMiezeCYIeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.named_parameters():\n",
        "#   print(name)\n",
        "  print(param)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGFpRnjJLCrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\\#  Working Code\n",
        "\n",
        "#  Initialising the initial mask with all ones\n",
        "mask_layerwise = {};\n",
        "for name, param in model.named_parameters():\n",
        "    mask_layerwise[name] = (torch.ones(param.size()).byte().to(device))\n",
        "\n",
        "\n",
        "# Pruning the weights \n",
        "for name, param in model.named_parameters():\n",
        "  if 'weight' in name:\n",
        "    sort_array = torch.sort(torch.abs(torch.masked_select(param, mask_layerwise[name]))).values\n",
        "    thres_index = int(percent*len(sort_array)/100)\n",
        "    threshold = sort_array[thres_index]\n",
        "    new_mask = param.ge(threshold).float()\n",
        "    new_param = param*new_mask\n",
        "    mask_layerwise[name] = new_mask        # Updating mask\n",
        "    param = new_param                      # Updating param\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRGZqj5wHvxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "  current_mask = torch.ones(param.size()).to(device)\n",
        "  param, new_mask = prune(prune_percent, param, current_mask)\n",
        "\n",
        "# Test the model after pruning\n",
        "model.eval()\n",
        "\n",
        "# TODO: Early Stopping\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        if total == 1000:\n",
        "            break\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))\n",
        "\n",
        "plt.plot(val_accuracy, label = \"Validation\")\n",
        "plt.plot(train_accuracy, label = \"Train\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oo5Q6O_6Bhwy",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x = torch.tensor([[1,2,3,4],[2,4,1,3],[5,8,2,4],[5,3,6,1],[6,4,2,6]]).float()\n",
        "percent = 20\n",
        "print(\"Original x\")\n",
        "print(x)\n",
        "print(torch.masked_select(x, mask))\n",
        "# current_mask = x.ge(-100)\n",
        "mask = torch.ones(x.size()).byte()\n",
        "x_sort_tensor = torch.sort(x)\n",
        "x_sort_array = torch.sort(torch.abs(torch.masked_select(x, mask))).values\n",
        "print(x_sort_array)\n",
        "thres_index = int(percent*len(x_sort_array)/100)\n",
        "threshold = x_sort_array[thres_index]\n",
        "print(thres_index)\n",
        "print(type(x))\n",
        "print(type(mask))\n",
        "print(type(torch.zeros(mask.shape)))\n",
        "new_mask = torch.where(torch.abs(x) <= threshold, torch.zeros(mask.shape).byte(), mask).float()\n",
        "# new_mask = x.ge(threshold).float()\n",
        "print(\"Threshold = \", threshold)\n",
        "print(\"New Mask\")\n",
        "print(new_mask)\n",
        "x_new = x*new_mask\n",
        "print(\"Pruned x\")\n",
        "print(x_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlNZZ1Ulr0uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "summary(model, (3, 32, 32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPGj-1wNsSLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}